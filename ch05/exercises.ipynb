{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e553f6e",
   "metadata": {},
   "source": [
    "# Chapter 5 — Exercises Notebook\n",
    "These are my notes and implementations for each exercise in Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e55d1",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities\n",
    "\n",
    "In this one, I'm testing how changing the temperature in softmax affects the model's word sampling. Lower temps make it more confident (less diverse), while higher temps give more variety. Trying out some values to see how often 'pizza' pops up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb309189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673669fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c2b41",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Different temperature and top-k settings\n",
    "\n",
    "Quick test here — just adjusting temperature and top-k to see how it changes outputs. It’s more about experimenting and watching the effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71b767",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Deterministic behavior in the decoding functions\n",
    "\n",
    "Trying to make the model's behavior more consistent when decoding. I’m making sure the seed is fixed so the outputs are deterministic. This way, results should match across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,       # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_chapters import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic function that used torch.argmax\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb32a5c",
   "metadata": {},
   "source": [
    "### Exercise 5.4: Continued pretraining\n",
    "\n",
    "Here, I'm continuing the pretraining with more data or steps. Just reloading the model and doing some additional epochs. Helps the model improve further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84caa50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_train import train_model_simple\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157e3e0",
   "metadata": {},
   "source": [
    "### Exercise 5.5: Training and validation set losses of the pretrained model\n",
    "\n",
    "This one’s about comparing training vs validation loss. I’m tracking both over time to make sure the model isn't overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "981ae9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:32:31.787115Z",
     "start_time": "2025-04-25T03:32:31.782233Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"01_main-chapter-code\")\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "ef1b7aa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:26.915901Z",
     "start_time": "2025-04-25T03:37:26.790158Z"
    }
   },
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_download\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m download_and_load_gpt2\n\u001B[0;32m      3\u001B[0m settings, params \u001B[38;5;241m=\u001B[39m download_and_load_gpt2(model_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m124M\u001B[39m\u001B[38;5;124m\"\u001B[39m, models_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\SWE\\LLMs-from-scratch\\ch05\\01_main-chapter-code\\gpt_download.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdownload_and_load_gpt2\u001B[39m(model_size, models_dir):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Validate model size\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "de222e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:31.468182Z",
     "start_time": "2025-04-25T03:37:30.637862Z"
    }
   },
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "b88ce1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:34.410752Z",
     "start_time": "2025-04-25T03:37:34.339197Z"
    }
   },
   "source": [
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_generate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_weights_into_gpt\n\u001B[0;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m load_weights_into_gpt(gpt, params)\n",
      "File \u001B[1;32m~\\SWE\\LLMs-from-scratch\\ch05\\01_main-chapter-code\\gpt_generate.py:12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# import requests\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtiktoken\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "3b8876fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:41.286345Z",
     "start_time": "2025-04-25T03:37:41.134063Z"
    }
   },
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "42e8e914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:47.268598Z",
     "start_time": "2025-04-25T03:37:46.612056Z"
    }
   },
   "source": [
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_train\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m calc_loss_loader\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m123\u001B[39m) \u001B[38;5;66;03m# For reproducibility due to the shuffling in the data loader\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m calc_loss_loader(train_loader, gpt, \u001B[43mdevice\u001B[49m)\n\u001B[0;32m      5\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m calc_loss_loader(val_loader, gpt, device)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_loss)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'device' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "a4e04fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:38:01.963963Z",
     "start_time": "2025-04-25T03:38:01.945006Z"
    }
   },
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_and_load_gpt2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m settings, params \u001B[38;5;241m=\u001B[39m \u001B[43mdownload_and_load_gpt2\u001B[49m(model_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1558M\u001B[39m\u001B[38;5;124m\"\u001B[39m, models_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-xl (1558M)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m NEW_CONFIG \u001B[38;5;241m=\u001B[39m GPT_CONFIG_124M\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'download_and_load_gpt2' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "9ba01e67",
   "metadata": {},
   "source": [
    "### Exercise 5.6: Trying larger models\n",
    "\n",
    "Final one. Scaling up. I’m switching to a larger model size to see how the loss improves. Bigger model = more capacity = probably better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b1fde28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:11.040193Z",
     "start_time": "2025-04-25T03:40:11.035954Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "d3c8a2eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:18.945126Z",
     "start_time": "2025-04-25T03:40:18.912318Z"
    }
   },
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "load_weights_into_gpt(gpt, params)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_download\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m download_and_load_gpt2\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_generate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_weights_into_gpt\n\u001B[0;32m      5\u001B[0m model_configs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-small (124M)\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memb_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m768\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m12\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m12\u001B[39m},\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-medium (355M)\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memb_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1024\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m24\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m16\u001B[39m},\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-large (774M)\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memb_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1280\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m36\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m20\u001B[39m},\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-xl (1558M)\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memb_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1600\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m48\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m25\u001B[39m},\n\u001B[0;32m     10\u001B[0m }\n",
      "File \u001B[1;32m~\\SWE\\LLMs-from-scratch\\ch05\\01_main-chapter-code\\gpt_download.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdownload_and_load_gpt2\u001B[39m(model_size, models_dir):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Validate model size\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "38267a0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:21.961340Z",
     "start_time": "2025-04-25T03:40:21.925180Z"
    }
   },
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_generate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generate, text_to_token_ids, token_ids_to_text\n",
      "File \u001B[1;32m~\\SWE\\LLMs-from-scratch\\ch05\\01_main-chapter-code\\gpt_generate.py:12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# import requests\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtiktoken\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "3c6937db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:25.900923Z",
     "start_time": "2025-04-25T03:40:25.885859Z"
    }
   },
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m123\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate\u001B[49m(\n\u001B[0;32m      4\u001B[0m     model\u001B[38;5;241m=\u001B[39mgpt,\n\u001B[0;32m      5\u001B[0m     idx\u001B[38;5;241m=\u001B[39mtext_to_token_ids(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvery effort moves you\u001B[39m\u001B[38;5;124m\"\u001B[39m, tokenizer),\n\u001B[0;32m      6\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,\n\u001B[0;32m      7\u001B[0m     context_size\u001B[38;5;241m=\u001B[39mNEW_CONFIG[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_length\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      8\u001B[0m     top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m      9\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.5\u001B[39m\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput text:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'generate' is not defined"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
